## function necessary to prepare texts from Gutenberg.org for analysis ##

PREP=function(x){
	x=x[(grep('start of this project', x, ignore.case=T)+1):(grep('end of this project', x, ignore.case=T)-1)]
	x=tolower(unlist(strsplit(x, '\\s|\n|\\W')))
	s=grep('^s$',x)
	x[s-1]=paste(x[s-1],'s',sep="'")
	x=x[grep('^s$',x, invert=T)]
	x=x[x!=''&x!='â'] 
x
}

## miller ##

#funtion to create a random text of specified length (in number of characters)
MILLER=function(length=10^6, prob=1){
	#alphabet consists of standard R object 'letters' and a wordbreak ("-")
	alphabet=c('-', letters)
	#if prob==1, all characters (including wordbreak) have equal probability, otherwise each character is selected with specified probabilities
	if(length(prob)==1){prob=rep(1, length(alphabet))}
	#generate text by sampling from alphabet
	text=paste(sample(alphabet, length, replace=T, prob=prob), collapse='')
	#identify words 
	text=unlist(strsplit(text, '-'))
	text=text[text!='']
#output sorted frequency list of words
sort(xtabs(~text), decreasing=T)
}

#Miller panel A
miller=MILLER(10^7)
plot(log10(1:length(miller)), log10(as.vector(miller)))
min=round(mean(grep(paste('^',min(miller),'$',sep=''), miller)))
abline(log10(min), -1, lwd=2, lty=2)

#after obtaining Melville's Moby Dick from gutenberg.org
moby.bk=scan("texts/moby.txt",what='char', sep='\n')
moby.txt=PREP(moby.bk)
moby=sort(xtabs(~moby.txt),decreasing=T)

#determine probability of characters in real text (Melville's Moby Dick)
millerProbs=paste(moby.txt, collapse=' ')
millerProbs=unlist(strsplit(millerProbs,''))
millerProbs=xtabs(~millerProbs)
millerProbs=millerProbs[names(millerProbs)%in%c(letters, ' ')]
millerProbs=round(prop.table(millerProbs), 3)

#Miller panel B
millerPlus=MILLER(10^7, prob=as.vector(millerProbs))
plot(log10(1:length(millerPlus)), log10(as.vector(millerPlus)))
min=round(mean(grep(paste('^',min(millerPlus),'$',sep=''), millerPlus)))
abline(log10(min), -1, lwd=2, lty=2)

###compare actual frequency of words with that predicted by Miller on the basis of character probability
##first set up data frame with actual frequencies and a starting "millerProb" of 1 (to be adjusted later)
millerTxt=data.frame(item=names(moby), rank=1:length(moby), freq=as.vector(moby), millerProb=1)
##next determine predicted probability for each word by multiplying probabilities of its characters
#for each character (excluding word break)
for(i in 2:length(millerProbs)){
	#identify words in which character appears ("grep()"), and adjust the predicted probability for these words by...
	millerTxt[grep(names(millerProbs)[i], millerTxt$item),]$millerProb=
	#rasing the probability of the character to the power of the number of times the character appears in each word, ... 
	millerProbs[i] ^ nchar(gsub(paste('[^',names(millerProbs)[i],']', sep=''),'',millerTxt[grep(names(millerProbs)[i], millerTxt$item),]$item)) * 
	#and then multiplying this with the probability the word was assigned already, on the basis of the other characters involved (with a starting probability 1)
	millerTxt[grep(names(millerProbs)[i], millerTxt$item),]$millerProb
}
#remove words with for which probability could not be determined 
millerTxt[millerTxt$millerProb==1,]$item
millerTxt=millerTxt[millerTxt$millerProb!=1,]
#determine predicted rank for each word on the basis of Miller
millerTxt=millerTxt[order(millerTxt$millerProb, decreasing=T),]
millerTxt$millerRank=1:nrow(millerTxt)
millerTxt=millerTxt[order(millerTxt$rank),]
#inspect results
head(millerTxt, 20)
cor(millerTxt$rank, millerTxt$millerRank)

#Miller panel C
plot(millerTxt$rank, millerTxt$millerRank)


## mandelbrot ##

#obtain Mandelbrot parameters
PARAMETERS=function(x){
	pars=lm(log10(x)~log10(1:length(x)))
	P=10^pars$coefficients[[1]]
	B=pars$coefficients[[2]]
	n=10^(-log10(P)/B)
	list(B, P, n)
}

#after downloading texts from gutenberg.org, prepare for analysis
moby.bk=scan("texts/moby.txt",what='char', sep='\n')
moby.txt=PREP(moby.bk)
moby=sort(xtabs(~moby.txt),decreasing=T)
sense.bk=scan("texts/sense.txt",what='char', sep='\n')
sense.txt=PREP(sense.bk)
sense=sort(xtabs(~sense.txt),decreasing=T)
twain.bk=scan("texts/twain.txt",what='char', sep='\n')
twain.txt=PREP(twain.bk)
twain=sort(xtabs(~twain.txt),decreasing=T)
joyce.bk=scan("texts/joyce.txt",what='char', sep='\n')
joyce.txt=PREP(joyce.bk)
joyce=sort(xtabs(~joyce.txt),decreasing=T)

#Mandelbrot panel A
plot(log10(1:length(moby)), log10(as.vector(moby)),xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim), col='grey60')
mobyPars=PARAMETERS(moby)
B=mobyPars[[1]]; P=mobyPars[[2]]; n=mobyPars[[3]]
m=5
freq=P*(m+(1:n))^B
lines(log10(1:n), log10(freq), lwd=2)
min=round(mean(grep(paste('^',min(moby),'$',sep=''), moby)))
lim=log10(max(moby[1], length(moby)))
abline(log10(min), -1, lwd=2, lty=2)

#Mandelbrot panel B
plot(log10(1:length(sense)), log10(as.vector(sense)),xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim), col='grey60')
sensePars=PARAMETERS(sense)
B=sensePars[[1]]; P=sensePars[[2]]; n=sensePars[[3]]
m=11
freq=P*(m+(1:n))^B
lines(log10(1:n), log10(freq), lwd=2)
min=round(mean(grep(paste('^',min(sense),'$',sep=''), sense)))
lim=log10(max(sense[1], length(sense)))
abline(log10(min), -1, lwd=2, lty=2)

#Mandelbrot panel C
plot(log10(1:length(twain)), log10(as.vector(twain)),xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim), col='grey60')
twainPars=PARAMETERS(twain)
B=twainPars[[1]]; P=twainPars[[2]]; n=twainPars[[3]]
m=8
freq=P*(m+(1:n))^B
lines(log10(1:n), log10(freq), lwd=2)
min=round(mean(grep(paste('^',min(twain),'$',sep=''), twain)))
lim=log10(max(twain[1], length(twain)))
abline(log10(min), -1, lwd=2, lty=2)

#Mandelbrot panel D
plot(log10(1:length(joyce)), log10(as.vector(joyce)),xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim), col='grey60')
joycePars=PARAMETERS(joyce)
B=joycePars[[1]]; P=joycePars[[2]]; n=joycePars[[3]]
m=1
freq=P*(m+(1:n))^B
lines(log10(1:n), log10(freq), lwd=2)
min=round(mean(grep(paste('^',min(joyce),'$',sep=''), joyce)))
lim=log10(max(joyce[1], length(joyce)))
abline(log10(min), -1, lwd=2, lty=2)




## Jaeger and van Rooij ##
JR=function(corpusSize=10^5, lexicon=10^5, oddsSameWord=7, contextWindow=100){
	lexicon=data.frame(ID=1:lexicon, frequency=0, rank=0, stringsAsFactors=F)
	#developing corpus
	print('developing corpus')
	corpus=vector()
	corpus[1]=sample(lexicon$ID, 1)
	lexicon[lexicon$ID==corpus[1],]$frequency=lexicon[lexicon$ID==corpus[1],]$frequency+1
	pb  <- txtProgressBar(max=corpusSize,style=3, char='=')
	for (i in 2:corpusSize){
		setTxtProgressBar(pb, i)
		nextWord=sample(c('new', 'same'), 1, prob=c(1, oddsSameWord))
		if(nextWord=='same'){
			if(length(corpus)<=contextWindow){context=corpus}
			if(length(corpus)>contextWindow){context=corpus[(length(corpus)-contextWindow):length(corpus)]}
			corpus[i]=sample(context, 1)
		}
		if(nextWord=='new'){
			corpus[i]=sample(lexicon$ID, 1)
		}
		lexicon[lexicon$ID==corpus[i],]$frequency=lexicon[lexicon$ID==corpus[i],]$frequency+1
	} 
	close(pb)
	lexicon=lexicon[order(lexicon$frequency, decreasing=T),]; lexicon$rank=1:nrow(lexicon)
	lim=max(log10(nrow(lexicon[lexicon$frequency!=0,])), log10(max(lexicon$frequency)))
	min=log10(mean(lexicon[lexicon$frequency==1,]$rank))
	plot(log10(lexicon$rank), log10(lexicon$frequency), xlim=c(0, lim), ylim=c(0,lim), ylab='log frequency', xlab='log rank', main='')
	abline(min, -1, lwd=2, lty=2)
	list(lexicon=lexicon, corpus=corpus)
}

coherence=JR(corpusSize=10^5, lexicon=10^5, oddsSameWord=7, contextWindow=100)
table=sort(xtabs(~coherence[[2]]), decreasing=T)

#non-overlap begin-end
corpus=coherence[[2]]
first=sort(xtabs(~corpus[1:10000]), decreasing=T)
last=sort(xtabs(~corpus[90001:100000]), decreasing=T)
top1=names(first)[1:50]
top2=names(last)[1:50]
intersect(top1, top2)


##Guiraud

GUIRAUD=function(lexicon){
	classSize=unique(lexicon$POS)
	for(i in 1:length(classSize)){
		nMeanings=nrow(lexicon[lexicon$POS==i,])
		nMeaningCols=ceiling(log(nMeanings, 3))
		guiraud=as.data.frame(replicate(nMeaningCols,rep(0,3^nMeaningCols)),stringsAsFactors=F)
		for(j in 1:nMeaningCols){guiraud[,j]=rep_len(sort(rep(c(0, 1, NA), 3^(j-1)), na.last=T), length.out=3^nMeaningCols)}
		guiraud=guiraud[rowSums(!is.na(guiraud))>0,]
		guiraud=guiraud[sample(nrow(guiraud), nMeanings),]
		lexicon[lexicon$POS==i,]$prob=round(.5^rowSums(!is.na(guiraud)),2)
	}
lexicon
}

## manin 

MANIN=function(dimensionality=9){
lexicon=as.data.frame(replicate(dimensionality,rep(0,2^dimensionality)),stringsAsFactors=F)
	for(i in 1:dimensionality){
		lexicon[,i]=rep_len(sort(rep(c(0, 1), 2^(i-1))), length.out=2^dimensionality)
	}
	add=lexicon
	for(i in dimensionality:2){
		add[,i]=NA
		add=unique(add)
		lexicon=rbind(lexicon, add)
	}
	lexicon$semanticWeight=(dimensionality-rowSums(is.na(lexicon)))/dimensionality
	lexicon$prob=1/lexicon$semanticWeight
	lexicon=lexicon[order(lexicon$prob, decreasing=T),]
	lexicon
}

manin=MANIN(19)