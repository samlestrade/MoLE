###############
## functions ##
###############

##ZIPF function parameters
#n is length of generated text in number of words
#classSize: vector specifying size of different POS classes
#classFrequency: vector specifying relative class frequency. If single dimension/value, classes are equally frequent.
#meaning: logical: should meaning be part of the simulation?
#nDims: number of meaning dimensions word can be specified for
#openClassThreshold: minimal number of elements a class should have in order to qualify as an open class
#nDistractors: number of distractor objects a word should distinguish its referent from
#output: logical: should a plot of the results be drawn?
ZIPF=function(n=10^6, classSize=c(5, 30, 50, 100, 500, 500, 1000, 15000, 25000, 100000), classFrequency=1, meaning=T, nDims=30, openClassThreshold=1, nDistractors=5, output=T){
	classSize=sort(classSize)
	#develop lexicon
	if(meaning==F){
		#if meaning is false, words are selected with equal probabilities from within POS classes
		lexicon=data.frame(ID=0, POS=rep(1:length(classSize), classSize), prob=1, freq=0, stringsAsFactors=F)
		for(i in 1:length(classSize)){lexicon$ID[lexicon$POS==i]=paste(i, 1:classSize[i], sep='-')}
	}
	if(meaning==T){
		#if meaning is true, first lexicon is set up with different word classes...
		lexicon=data.frame(ID=0, POS=rep(1:length(classSize), classSize), spec=0, prob=0, freq=0, pApplicable=NA, pNoDistractors=1, stringsAsFactors=F)
		for(i in 1:length(classSize)){
			lexicon$ID[lexicon$POS==i]=paste(i, 1:classSize[i], sep='-')
		}
		#next, words are assigned a specifity value on the basis of the random number of meaning dimensions they are specified for. The maximum number of dimensions is set by the nDims parameter, odds for being specified for a meaning dimension are 1:2.
		lexicon$spec=replicate(nrow(lexicon), sum(sample(c(0:1), nDims, replace=T, prob=c(2,1))))
		#prob that item is applicable to any given object is .5 to the number of dimensions it is specified for (given binary dimensions).
		lexicon$pApplicable=.5^lexicon$spec	
		for(i in 1:length(unique(lexicon$POS))){
			#prob that there is no distractor to which an item from an open class could apply (closed class items have no distractors)
			if(classSize[i]>=openClassThreshold){lexicon[lexicon$POS==i,]$pNoDistractors=(1-lexicon[lexicon$POS==i,]$pApplicable)^nDistractors}	
		}
		#prob for an item to be used is prob that it is applicable in principle times the prob it has no distractors
		lexicon$prob=lexicon$pApplicable*lexicon$pNoDistractors
	}
	##develop corpus
	#set up class probabilities if classes are equiprobable
	if(length(classFrequency)==1){classFrequency=rep(classFrequency, length(classSize))}
	#build corpus by sampling items from word classes with specified probs (which are all the same if meaning=F), with total class frequencies as specified by classFrequency parameter
	corpus=vector()
	for (i in 1:length(classSize)){
		corpus=c(corpus, sample(lexicon[lexicon$POS==i,]$ID, n/sum(classFrequency)*classFrequency[i], replace=T, prob=lexicon[lexicon$POS==i,]$prob))
	}
	#determine usage numbers
	corpus=table(corpus)
	#mark frequency of each word in lexicon
	#NB frequency is determined by word ID, hence homonymy and/or polysemy do not apply
	lexicon$freq[match(names(corpus), lexicon$ID)]=corpus
	#order corpus by frequency and determine  rank
	lexicon=lexicon[order(lexicon$freq, decreasing=T),]
	lexicon$rank=1:nrow(lexicon)
	if(output==T){
		#plot results
		zipf=lexicon[lexicon$freq>0,]
		lim=log10(max(zipf$freq, zipf$rank))
		min=round(mean(grep(paste('^',min(zipf$freq),'$',sep=''), zipf$freq)))
		plot(log10(zipf$rank), log10(zipf$freq), xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim))
		abline(log10(min), -1, lwd=2, lty=2)
	}
#output lexicon
lexicon
}

#function to determine total frequency of wordnet noun(s) in Brown corpus
BROWN=function(){
	#brown corpus should be in R working space
	corpus=brown[brown$pos=='N',]
	#queries are wordnet nouns, which may involve synonyms
	queries=gsub('^\\s+','',tolower(unlist(strsplit(as.character(wordnet$noun), ', '))))
	freqs=corpus$freq[match(queries, corpus$type)]
	wordnet$freq=0
	freq=1
	pb  <- txtProgressBar(max=nrow(wordnet),style=3, char='*', width=round(.5*getOption("width")))
	for(i in 1:nrow(wordnet)){
		n=wordnet$nSynonyms2[i]
		#add up frequencies of synonyms per entry
		wordnet$freq[i]=sum(freqs[freq:(freq+n-1)], na.rm=T)
		freq=freq+n
		setTxtProgressBar(pb, i)
	}
	close(pb)
wordnet
}


#Determine proportion of matching specifications of (possibly underspecified) representations with (fully specified) target meaning. Unspecified dimensions count as a match (i.e., a non-mismatch)
VMATCH=function(target, representations){
	representations=representations[1:length(target)]
	z=as.data.frame(t(replicate(nrow(representations),as.numeric(target))), stringsAsFactors=F)
	diffs=abs(z-representations)
	vmatch=1-(rowSums(diffs, na.rm=T)/length(target))
vmatch
}


#function to set up meaning space 
ROSCH=function(nWords=1000, nDimensions=10, nSituations=10000, nDistractors=5){
	print(paste("Developing lexicon and usage frequencies")); flush.console()
	#set up lexicon with nDimenions meaning dimensions
	lexicon=as.data.frame(replicate(nDimensions,sample(c(NA, 0, 1), nWords, replace=T)),stringsAsFactors=F)
	#degree of specification is number of meaning dimensions for which an element is specified
	lexicon$ID=1:nWords; lexicon$spec=rowSums(!is.na(lexicon[,1:nDimensions])); lexicon$freq=0
	##calculate prob of usage
	#p that a word is aplicable (assuming binary specifications) is .5 to the power of the number of meaning dimensions it is specified for
	lexicon$pApplicable=(1/2)^lexicon$spec
	#p that a word has is distinctive given nDistractors distractors is 1 - p that it describes a distractor to the power of the number of distractors
	lexicon$pNoDistractors=(1-(1/2)^lexicon$spec )^nDistractors
	#prob that a word is used is p that it is applicable in principle times p that it does not apply to a distractor
	lexicon$prob=lexicon$pApplicable*lexicon$pNoDistractors
	##simulate freq of usage
	pb  <- txtProgressBar(max=nSituations,style=3, char='*', width=round(.5*getOption("width")))
	for(n in 1:nSituations){
		#generate target and situation in which target has to be identified
		situation=as.data.frame(replicate(nDimensions,sample(0:1, nDistractors, replace=T)),stringsAsFactors=F)
		target=sample(0:1, nDimensions, replace=T)
		#determine match of lexemes with target referent
		lexicon$match=VMATCH(target, lexicon)
		#randomize lexicon
		lexicon=lexicon[sample(nrow(lexicon)),]
		#order lexemes by match with referent
		lexicon=lexicon[order(lexicon$match, decreasing=T),]
		#start with best match, and check if sufficiently distinctive given situation. If so, update frequency and generate new context
		for (i in 1:nWords){	
			distractorMatch=max(VMATCH(lexicon[i,grep('^V\\d',names(lexicon))], situation))
			if(lexicon[i,]$match > distractorMatch){
				lexicon[i,]$freq=lexicon[i,]$freq+1
				break()
		}	}
		setTxtProgressBar(pb, n)
	}
	close(pb)
	lexicon=lexicon[order(lexicon$freq, decreasing=T),]
	lexicon$rank=1:nrow(lexicon)
lexicon
}


#function to prepare Gutenberg texts for word count
PREP=function(x){
	x=x[(grep('start of this project', x, ignore.case=T)+1):(grep('end of this project', x, ignore.case=T)-1)]
	x=tolower(unlist(strsplit(x, '\\s|\n|\\W')))
	s=grep('^s$',x)
	x[s-1]=paste(x[s-1],'s',sep="'")
	x=x[grep('^s$',x, invert=T)]
	x=x[x!=''&x!='Ã¢'] 
x
}


############
## output ##
############

##if data objects are not already part of working space, load these first
#load preprocessed WordNet
wordnet=read.csv2('PATH/wordnet.csv', T)
#load preprocessed Brown
wordnet=read.csv2('PATH/brown.rdata', T)
#load preprocessed HNC
hnc=read.csv2('PATH/magy.csv', T)
#load preprocessed CGN
wordnet=read.csv2('PATH/cgn.rdata', T)

#after downloading Moby Dick from Gutenberg, load text
moby.bk=scan("texts/moby.txt",what='char', sep='\n')
moby.txt=PREP(moby.bk)
moby=sort(xtabs(~moby.txt),decreasing=T)


## FIGURE 1 ##
##to save as tiff file, remove comment characters on next and last line
#tiff(filename='plots/Fig1.tif', width=2250, height=789, res=300, units='px',pointsize=11,bg='white',compression='lzw',restoreConsole=TRUE)
#set up 3-panel plot
par(mfrow=c(1,3))
#plot simple rank-frequency relation
n=1:1000; freq=1000/n
plot(n, freq, xlab='rank', ylab='frequency')
#plot relation in double log space
plot(log10(n), log10(freq), xlab='log rank', ylab='log frequency')
#plot word counts for Moby Dick
lim=log10(max(moby[1], length(moby)))
plot(log10(1:length(moby)), log10(as.vector(moby)),xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim))
min=round(mean(grep(paste('^',min(moby),'$',sep=''), moby)))
abline(log10(min), -1, lwd=2, lty=2)
#dev.off()

## TABLE 1 ##
#cgn
cgnTable=data.frame(pos=names(table(cgn$pos)), freq=tapply(cgn$freq, cgn$pos, sum), size=as.vector(table(cgn$pos)), stringsAsFactors=F, row.names=NULL)
cgnTable$freq=round(cgnTable$freq/sum(cgn$freq)*100,2)
(cgnTable=cgnTable[order(cgnTable$freq/cgnTable$size, decreasing=T),])
#brown
brownTable=data.frame(pos=names(table(brown$pos)), freq=tapply(brown$freq, brown$pos, sum), size=as.vector(table(brown$pos)), stringsAsFactors=F, row.names=NULL)
brownTable$freq=round(brownTable$freq/sum(brown$freq)*100,2)
brownTable=head(brownTable[order(brownTable$freq, decreasing=T),], 10)
(brownTable=brownTable[order(brownTable$freq/brownTable$size, decreasing=T),])
#hnc
hncTable=data.frame(pos=names(table(hnc$pos)), freq=tapply(hnc$freq, hnc$pos, sum), size=as.vector(table(hnc$pos)), stringsAsFactors=F, row.names=NULL)
hncTable$freq=round(hncTable$freq/sum(hnc$freq)*100,2)
hncTable=head(hncTable[order(hncTable$freq, decreasing=T),], 10)
(hncTable=hncTable[order(hncTable$freq/hncTable$size, decreasing=T),])

## FIGURE 2 ##
#tiff(filename='plots/Fig2.tif', width=1578, height=1578, res=300, units='px',pointsize=11,bg='white',compression='lzw',restoreConsole=TRUE)
par(mfrow=c(1,1))
syntax=ZIPF(classSize=cgnTable$size, classFrequency=cgnTable$freq, meaning=F, output=F)
syntax=syntax[syntax$freq>0,]
lim=log10(max(syntax$freq, syntax$rank))
min=round(mean(grep(paste('^',min(syntax$freq),'$',sep=''), syntax$freq)))
plot(log10(syntax$rank), log10(syntax$freq), type='n', xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim))
text(log10(syntax$rank), log10(syntax$freq), syntax$POS)
legend('topright', legend=c('word class 1', 'word class 2', 'etc.'), pch=c('1', '2', ''))
abline(log10(min), -1, lwd=2, lty=2)
#dev.off()

## FIGURE 3 ##
#BROWN() checks for each concept how frequent it is in the Brown corpus, cf. function description above.
wordnet=BROWN()
wordnet=wordnet[wordnet$freq>0,]
wordnet=wordnet[order(wordnet$freq, decreasing=T),]
wordnet$rank=1:nrow(wordnet)
#show "thing" and "submarine"
wordnet[wordnet$ID%in%c('4347754', '2452'),]
##if wordnet is created in current session and not reloaded, use next line (IDs are of equal length and start with zeros)
#wordnet[wordnet$ID%in%c('04347754', '00002452'),]
wordnet$embedding=ifelse(wordnet$spec%in%3:9, 'medium', 'high/low')
#tiff(filename='plots/Fig3.tif', width=1578, height=2250, res=300, units='px',pointsize=11,bg='white',compression='lzw',restoreConsole=TRUE)
par(mfrow=c(2,1))
plot(log10(wordnet$rank), log10(wordnet$freq), xlab='log rank', ylab='log freq', main='', type='n')
points(log10(wordnet[wordnet$embedding=='medium',]$rank), log10(wordnet[wordnet$embedding=='medium',]$freq), pch=1, col='red')
points(log10(wordnet[wordnet$embedding!='medium',]$rank), log10(wordnet[wordnet$embedding!='medium',]$freq), pch=3, col='blue')
legend('topright', legend=c('medium', 'high/low'), pch=c(1,3), col=c('red', 'blue'))
boxplot(log10(wordnet$rank)~wordnet$embedding, notch=T, outline=T, ylab='embedding', xlab='log rank', horizontal=T)
#dev.off()

## FIGURE 4 ##
simulation=ROSCH()
simulation$specClass=ifelse(simulation$spec%in%2:4, 'medium', 'high/low')
#tiff(filename='plots/Fig4.tif', width=1578, height=2250, res=300, units='px',pointsize=11,bg='white',compression='lzw',restoreConsole=TRUE)
par(mfrow=c(2,1))
plot(log10(simulation$rank), log10(simulation$freq), col='lightgrey', xlim=c(0, log10(nrow(simulation))), ylim=c(0, log10(max(simulation$freq))), xlab='log rank', ylab='log freq', main='')
points(log10(simulation[simulation$specClass=='medium',]$rank), log10(simulation[simulation$specClass=='medium',]$freq), pch=1, col='red')
points(log10(simulation[simulation$specClass!='medium',]$rank), log10(simulation[simulation$specClass!='medium',]$freq), pch=3, col='blue')
legend('topright', legend=c('medium', 'high/low'), pch=c(1,3), col=c('red', 'blue'))
boxplot(log10(simulation$rank)~simulation$specClass, notch=T, outline=T, ylab='specificity class', xlab='log rank', horizontal=T)
#dev.off()

## FIGURE 5 ##
model=ZIPF(n=10^4, classSize=1000)
model$specClass=ifelse(model$spec%in%3:6, 'medium', 'high/low')
#tiff(filename='plots/Fig5.tif', width=1578, height=2250, res=300, units='px',pointsize=11,bg='white',compression='lzw',restoreConsole=TRUE)
par(mfrow=c(2,1))
plot(log10(model$rank), log10(model$freq), col='lightgrey', xlim=c(0, log10(nrow(model))), ylim=c(0, log10(max(model$freq))), xlab='log rank', ylab='log freq', main='')
points(log10(model[model$specClass=='medium',]$rank), log10(model[model$specClass=='medium',]$freq), pch=1, col='red')
points(log10(model[model$specClass!='medium',]$rank), log10(model[model$specClass!='medium',]$freq), pch=3, col='blue')
legend('topright', legend=c('medium', 'high/low'), pch=c(1,3), col=c('red', 'blue'))
boxplot(log10(model$rank)~model$specClass, notch=T, outline=T, ylab='specificity class', xlab='log rank', horizontal=T)
#dev.off()

## FIGURE 6 ##
zipf=ZIPF(output=F)
zipf=zipf[zipf$freq>0,]
lim=log10(max(zipf$freq, zipf$rank))
min=round(mean(grep(paste('^',min(zipf$freq),'$',sep=''), zipf$freq)))
#tiff(filename='plots/Fig6.tif', width=1578, height=1578, res=300, units='px',pointsize=11,bg='white',compression='lzw',restoreConsole=TRUE)
par(mfrow=c(1,1))
plot(log10(zipf$rank), log10(zipf$freq), xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim), type='n')
text(log10(zipf$rank), log10(zipf$freq), zipf$POS)
abline(log10(min), -1, lwd=2, lty=2)
#dev.off()


## FIGURE 7 ##
zipfCC=ZIPF(n=sum(cgn$freq), classSize=cgnTable$size, classFrequency=cgnTable$freq)
zipfCB=ZIPF(n=sum(cgn$freq), classSize=brownTable$size, classFrequency=brownTable$freq)
zipfBB=ZIPF(n=sum(brown$freq), classSize=brownTable$size, classFrequency=brownTable$freq)
zipfBC=ZIPF(n=sum(brown$freq), classSize=cgnTable$size, classFrequency=cgnTable$freq)
#tiff(filename='plots/Fig7.tif', width=2250, height=1578, res=300, units='px',pointsize=11,bg='white',compression='lzw',restoreConsole=TRUE)
par(mfrow=c(1,2))
#cgn
lim=log10(max(cgn$freq, cgn$rank))
min=round(mean(grep(paste('^',min(cgn$freq),'$',sep=''), cgn$freq)))
plot(log10(cgn$rank), log10(cgn$freq), xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim), col='grey')
abline(log10(min), -1, lwd=2, lty=2)
points(log10(zipfCC$rank), log10(zipfCC$freq), pch=2, col='blue')
points(log10(zipfCB$rank), log10(zipfCB$freq), pch=3, col='red')
#brown
lim=log10(max(brown$freq, brown$rank))
min=round(mean(grep(paste('^',min(brown$freq),'$',sep=''), brown$freq)))
plot(log10(brown$rank), log10(brown$freq), xlab='log rank',ylab='log frequency',cex.lab=1,lwd=2, xlim=c(0, lim), ylim=c(0, lim), col='grey')
abline(log10(min), -1, lwd=2, lty=2)
points(log10(zipfBB$rank), log10(zipfBB$freq), pch=2, col='blue')
points(log10(zipfBC$rank), log10(zipfBC$freq), pch=3, col='red')
#dev.off()

## Class-size predictions ##
sum(moby)
sum(moby[names(moby)%in%c('the','an','a')])/sum(moby)
length(moby)
